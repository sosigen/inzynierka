{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense, Dropout, Concatenate, Add, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from spektral.layers import GlobalAvgPool, GlobalMaxPool, GCNConv\n",
    "\n",
    "# New callback to log gradient norms\n",
    "class GradientLogger(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Get one batch from the dataset\n",
    "        for batch in self.dataset.take(1):\n",
    "            inputs, targets = batch\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = self.model(inputs, training=True)\n",
    "                # Use the model's compiled loss function\n",
    "                loss = self.model.compiled_loss(targets, predictions)\n",
    "            gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "            norm_list = []\n",
    "            for grad in gradients:\n",
    "                if grad is not None:\n",
    "                    norm_list.append(tf.norm(grad).numpy())\n",
    "            if norm_list:\n",
    "                avg_norm = np.mean(norm_list)\n",
    "                max_norm = np.max(norm_list)\n",
    "                min_norm = np.min(norm_list)\n",
    "                print(f\"Epoch {epoch+1} - Gradients: Mean L2 norm: {avg_norm:.6f}, Max: {max_norm:.6f}, Min: {min_norm:.6f}\")\n",
    "            break\n",
    "\n",
    "class NoMaskGCNConv(GCNConv):\n",
    "    def call(self, inputs, mask=None):\n",
    "        return super().call(inputs, mask=None)\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return None\n",
    "\n",
    "def build_revised_gcn_model(n_node_features, n_classes, num_nodes):\n",
    "    # Inputs: node features and adjacency matrix\n",
    "    x_in = tf.keras.Input(shape=(num_nodes, n_node_features), name=\"node_features\")\n",
    "    a_in = tf.keras.Input(shape=(num_nodes, num_nodes), name=\"adjacency_matrix\")\n",
    "    \n",
    "    # --- GCN Layer 1 ---\n",
    "    x1 = NoMaskGCNConv(32, activation='relu')([x_in, a_in])\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = Dropout(0.2)(x1)\n",
    "\n",
    "    # --- GCN Layer 2 ---\n",
    "    x2 = NoMaskGCNConv(64, activation='relu')([x1, a_in])\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = Dropout(0.2)(x2)\n",
    "\n",
    "    # --- GCN Layer 3 ---\n",
    "    x3 = NoMaskGCNConv(128, activation='relu')([x2, a_in])\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3 = Dropout(0.2)(x3)\n",
    "\n",
    "    # --- Residual Connection ---\n",
    "    x2_proj = Dense(128, activation='linear')(x2)\n",
    "    x3 = Add()([x3, x2_proj])\n",
    "\n",
    "    # --- Global Pooling ---\n",
    "    x_max = GlobalMaxPool()(x3)\n",
    "    x_avg = GlobalAvgPool(name='global_avg_pool')(x3)\n",
    "    x = Concatenate()([x_max, x_avg])\n",
    "    \n",
    "    # --- Fully Connected Layers ---\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    # Final classification layer\n",
    "    output = Dense(n_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=[x_in, a_in], outputs=output)\n",
    "    return model\n",
    "\n",
    "img_size = 48\n",
    "patch_size = 6\n",
    "num_nodes = (img_size // patch_size) ** 2  # 48/6 = 8 -> 8x8 grid = 64 nodes\n",
    "n_node_features = patch_size * patch_size + 2  # 36 + 2 = 38\n",
    "n_classes = 7 \n",
    "\n",
    "model = build_revised_gcn_model(n_node_features=n_node_features, n_classes=n_classes, num_nodes=num_nodes)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "gradient_logger = GradientLogger(train_dataset)\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    validation_data=validation_dataset,\n",
    "    validation_steps=len(validation_generator),\n",
    "    epochs=20,\n",
    "    callbacks=[early_stop, gradient_logger]\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
