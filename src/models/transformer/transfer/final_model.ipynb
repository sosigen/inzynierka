{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_keras import vit\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "# 1) Create your base ViT model (pretrained)\n",
    "base_model = vit.vit_b32(\n",
    "    image_size=img_size,\n",
    "    pretrained=True,\n",
    "    include_top=False,\n",
    "    pretrained_top=False\n",
    ")\n",
    "\n",
    "# 2) Freeze the entire base model first\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# 3) Create your classifier head\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(Dense(1024, activation='gelu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(len(emotion_labels), activation='softmax'))\n",
    "\n",
    "# 4) Compile with base_model frozen - only the Dense head trains\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.AdamW(learning_rate=1e-4, weight_decay=1e-5),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 5) Callbacks\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath='ViT_pretrained_head_only.keras',\n",
    "                                       monitor='val_loss',\n",
    "                                       save_best_only=True)\n",
    "]\n",
    "\n",
    "# 6) Train just the Dense head for a few epochs\n",
    "history_head = model.fit(\n",
    "    train_generator,\n",
    "    epochs=5,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Unfreeze some of the last layers of the base model\n",
    "for layer in base_model.layers[-8:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "\n",
    "# 8) Re-compile after unfreezing\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.AdamW(learning_rate=1e-5, weight_decay=1e-5),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "callbacks_finetune = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath='ViT_pretrained_finetuned.keras',\n",
    "                                       monitor='val_loss',\n",
    "                                       save_best_only=True)\n",
    "]\n",
    "\n",
    "# 10) Fine-tune the model\n",
    "history_finetune = model.fit(\n",
    "    train_generator,\n",
    "    epochs=15,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=callbacks_finetune\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
